{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add580c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textract\n",
      "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.8/site-packages (3.4.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.9.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.1)\n",
      "Collecting argcomplete~=1.10.0\n",
      "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting python-pptx~=0.6.18\n",
      "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer.six==20191110\n",
      "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docx2txt~=0.8\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "Collecting extract-msg<=0.29.*\n",
      "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 38.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting SpeechRecognition~=3.8.1\n",
      "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 32.8 MB 18.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4~=4.8.0\n",
      "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting xlrd~=1.2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 52.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.15.0-cp35-abi3-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer.six==20191110->textract) (2.3.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4~=4.8.0->textract) (2.2.1)\n",
      "Collecting ebcdic>=1.1.1\n",
      "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 96.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzlocal>=2.1\n",
      "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Collecting compressed-rtf>=1.0.6\n",
      "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: olefile>=0.46 in /opt/anaconda3/lib/python3.8/site-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
      "Collecting imapclient==2.1.0\n",
      "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 18.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from python-pptx~=0.6.18->textract) (4.6.3)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /opt/anaconda3/lib/python3.8/site-packages (from python-pptx~=0.6.18->textract) (8.2.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/anaconda3/lib/python3.8/site-packages (from python-pptx~=0.6.18->textract) (1.3.8)\n",
      "Collecting backports.zoneinfo\n",
      "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-macosx_10_14_x86_64.whl (35 kB)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2022.2-py2.py3-none-any.whl (336 kB)\n",
      "\u001b[K     |████████████████████████████████| 336 kB 48.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, python-pptx\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3966 sha256=a8c0e69dc5639d6c86202222739c3bb1a22adf5e2071ec7775511a9f6492acb1\n",
      "  Stored in directory: /Users/fanyunjung/Library/Caches/pip/wheels/55/f0/2c/81637d42670985178b77df6d41b9b6c6dc18c94818447414b9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6209 sha256=af2276357e4721b1e7866fed9dd66d5785713695bff523eb1a38c9e1ef45c7a4\n",
      "  Stored in directory: /Users/fanyunjung/Library/Caches/pip/wheels/11/f5/c4/81acab65ab073b5a3e67fd82e4b9accf3dbcf1de39c7b246ec\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=471172 sha256=0101ad8499c83f0934880d6537a9c27ad3c5c8c7a311c4d23e5154ae1dd94779\n",
      "  Stored in directory: /Users/fanyunjung/Library/Caches/pip/wheels/b0/38/58/8530ed1681bfee42349acf166867cc9fb369517b2fce83e599\n",
      "Successfully built docx2txt compressed-rtf python-pptx\n",
      "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, tzlocal, pycryptodome, imapclient, ebcdic, compressed-rtf, chardet, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: xlrd\n",
      "    Found existing installation: xlrd 2.0.1\n",
      "    Uninstalling xlrd-2.0.1:\n",
      "      Successfully uninstalled xlrd-2.0.1\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.9.3\n",
      "    Uninstalling beautifulsoup4-4.9.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.9.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 4.2.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\u001b[0m\n",
      "Successfully installed SpeechRecognition-3.8.1 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 pdfminer.six-20191110 pycryptodome-3.15.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 textract-1.6.5 tzdata-2022.2 tzlocal-4.2 xlrd-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn spacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d3b8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading PyPDF2-2.10.7-py3-none-any.whl (217 kB)\n",
      "\u001b[K     |████████████████████████████████| 217 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.10.0.0\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: typing-extensions, PyPDF2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "Successfully installed PyPDF2-2.10.7 typing-extensions-4.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 #library to read pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144f6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web-scrapping libraries\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import csv\n",
    "import json\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import traceback\n",
    "import pyautogui as pag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70e3e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fanyunjung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fanyunjung/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# pdf reading and nlp libraries\n",
    "import PyPDF2 \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "all_stopwords = stopwords.words('english')\n",
    "import ssl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9392f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the full-time data analyst position with entry level just posted a month ago\n",
    "url = \"https://www.linkedin.com/jobs/search/\\\n",
    "?currentJobId=3215752036&f_E=2&f_JT=F&f_TPR=r2592000&geoId=103644278&keywords=data%20analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "acd79ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-176-fbfc118e445e>:117: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/fanyunjung/Desktop/Others/Project/chromedriver',options = options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to collect information on /html/body/div[1]/div/main/section[2]/ul/li[984]/div/div[1]/img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-176-fbfc118e445e>\", line 99, in getLinkedinJobs\n",
      "    job_click = job.find_element(By.XPATH,job_click_path)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webelement.py\", line 718, in find_element\n",
      "    return self._execute(Command.FIND_CHILD_ELEMENT,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webelement.py\", line 693, in _execute\n",
      "    return self._parent.execute(command, params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 418, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py\", line 243, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div/main/section[2]/ul/li[984]/div/div[1]/img\"}\n",
      "  (Session info: chrome=105.0.5195.102)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100ad2ae0 chromedriver + 3828448\n",
      "1   chromedriver                        0x0000000100a67f1c chromedriver + 3391260\n",
      "2   chromedriver                        0x0000000100760fcc chromedriver + 217036\n",
      "3   chromedriver                        0x0000000100790b7c chromedriver + 412540\n",
      "4   chromedriver                        0x0000000100787308 chromedriver + 373512\n",
      "5   chromedriver                        0x00000001007b9b2c chromedriver + 580396\n",
      "6   chromedriver                        0x0000000100786010 chromedriver + 368656\n",
      "7   chromedriver                        0x0000000100aa839c chromedriver + 3654556\n",
      "8   chromedriver                        0x0000000100aabc4c chromedriver + 3669068\n",
      "9   chromedriver                        0x0000000100ab014c chromedriver + 3686732\n",
      "10  chromedriver                        0x0000000100aac654 chromedriver + 3671636\n",
      "11  chromedriver                        0x0000000100a8ab40 chromedriver + 3533632\n",
      "12  chromedriver                        0x0000000100ac4414 chromedriver + 3769364\n",
      "13  chromedriver                        0x0000000100ac4578 chromedriver + 3769720\n",
      "14  chromedriver                        0x0000000100ad90f0 chromedriver + 3854576\n",
      "15  libsystem_pthread.dylib             0x000000018d7ff878 _pthread_start + 320\n",
      "16  libsystem_pthread.dylib             0x000000018d7fa5e0 thread_start + 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getJobInfo(job):\n",
    "    title = ''\n",
    "    company = ''\n",
    "    location = ''\n",
    "    time = ''\n",
    "    jobLink = ''\n",
    "    jobDescription = ''\n",
    "\n",
    "    try:\n",
    "        title = job.find_element(By.CSS_SELECTOR,'#main-content > section.two-pane-serp-page__results-list > ul > li > div > div.base-search-card__info > h3').text.strip(\"\\n  \")\n",
    "    except:\n",
    "        title = 'NA'\n",
    "    try:\n",
    "        company = job.find_element(By.CSS_SELECTOR,'#main-content > section.two-pane-serp-page__results-list > ul > li > div > div.base-search-card__info > h4 > a').text.strip(\"\\n  \")\n",
    "    except:\n",
    "        company = 'NA'\n",
    "    try:\n",
    "        location = job.find_element(By.CSS_SELECTOR,'#main-content > section.two-pane-serp-page__results-list > ul > li > div > div.base-search-card__info > div > span').text.strip(\"\\n  \")\n",
    "    except:\n",
    "        location = 'NA'\n",
    "    try:\n",
    "        time = job.find_element(By.CSS_SELECTOR, '#main-content > section.two-pane-serp-page__results-list > ul > li > div > div.base-search-card__info > div > time').get_attribute('datetime')\n",
    "    except:\n",
    "        time = 'NA'\n",
    "    try:\n",
    "        jobLink = job.find_element(By.CSS_SELECTOR,'#main-content > section.two-pane-serp-page__results-list > ul > li > div > a').get_attribute('href')\n",
    "    except:\n",
    "        jobLink = 'NA'\n",
    "    try:\n",
    "        job_desc = driver.find_element(By.CLASS_NAME,'show-more-less-html')\n",
    "        jobDescription = BeautifulSoup(job_desc.get_attribute('outerHTML'), 'html.parser').text.replace('\\n',' ')\n",
    "    except:\n",
    "        jobDescription = 'NA'\n",
    "    return [title, company, location, time, jobLink, jobDescription]\n",
    "\n",
    "def scroll():\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i < 100:\n",
    "        for j in range(5):\n",
    "            pag.scroll(-10)\n",
    "        sleep(1)\n",
    "        i=i+1\n",
    "        \n",
    "        try:\n",
    "            #try to find the \"See more Jobs\" button\n",
    "            driver.find_element(By.CSS_SELECTOR,'#main-content > section.two-pane-serp-page__results-list > button').click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def getLinkedinJobs():\n",
    "    driver.get(url)\n",
    "    sleep(5) \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scroll() #scroll down to make all jobs show up first\n",
    "    actions = ActionChains(driver)\n",
    "    jobData = [['title','company','location', 'time', 'jobLink', 'jobDescription']]\n",
    " \n",
    "    #catch every job\n",
    "    jobs = driver.find_elements(By.CSS_SELECTOR,'#main-content > section.two-pane-serp-page__results-list > ul > li') \n",
    "    \n",
    "    i=0\n",
    "\n",
    "    for job in jobs:\n",
    "        i+=1\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:  \n",
    "            job_click_path = f'/html/body/div[1]/div/main/section[2]/ul/li[{i}]/div/div[1]/img' #click on the image so that we can get more details\n",
    "            job_click = job.find_element(By.XPATH,job_click_path)\n",
    "            actions.move_to_element(job_click).click().perform()\n",
    "            time.sleep(5)\n",
    "            jobData.append(getJobInfo(job))\n",
    "           \n",
    "        except:\n",
    "            print(f'unable to collect information on {job_click_path}') #print the path of image that couldn't click\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(jobData[1:],columns=jobData[0])\n",
    "    #Drop any duplicate\n",
    "    df.drop_duplicates(subset = ['jobLink'],inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(executable_path='/Users/fanyunjung/Desktop/Others/Project/chromedriver',options = options)  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    df = getLinkedinJobs()\n",
    "    df.to_excel(\"linkedIn.xlsx\") #save in an Excel file for tracking easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c777f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "    text = ' '.join(tokens_without_sw)\n",
    "    return text\n",
    "\n",
    "def resumeReader(pdf):\n",
    "    # Read Resume\n",
    "    with open(pdf, \"rb\") as pdf_file:\n",
    "        resume = PyPDF2.PdfFileReader(pdf).getPage(0).extractText()\n",
    "    return resume\n",
    "\n",
    "def score(resume,jobDescription):\n",
    "    cv = CountVectorizer()\n",
    "    resume = ' '.join(set(cleanText(resume).split(' ')))\n",
    "    jobDescription = ' '.join(set(cleanText(jobDescription).split(' ')))\n",
    "    text = [resume,jobDescription]\n",
    "    count_matrix = cv.fit_transform(text)\n",
    "    return round(cosine_similarity(count_matrix)[0][1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ce95520",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = \"/Users/fanyunjung/Desktop/Others/Project/Resume_Yun-Jung.pdf\"\n",
    "resume = resumeReader(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1792905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"YUN-JUNG FAN ● Hyattsville, MD ● (301) 520-5458 ● yfan1234@umd.edu ● https://www.linkedin.com/in/yunjungfan/  SUMMARY Business analyst with three years of experience. Familiar with collecting and processing data via Python, and visualizing results with Tableau. Seek to utilize my analytical skills and business acumen to extract data, recognize insights, and solve real-world problems.  CORE COMPETENCIES • Data Collection • Financial Analysis • Client Relations • Data Analysis • Attention To Detail • Team Collaboration • Data Visualization • Time Management • Problem-Solving  EDUCATION  Robert H. Smith School of Business, University of Maryland, College Park                                                       College Park, MD, USA Master of Business Analytics (GPA: 3.92/4.0)                                                                                  September 2021 – Present • Relevant Coursework: Database Management Systems, Data Processing and Analytics in Python, Data Mining and Predictive Analytics, Data Visualization and Web Analytics  National Central University                                                                                                                                   Taoyuan, Taiwan  Bachelor of Business Administration (GPA: 3.74/4.0)                                                           September 2013 – June 2017 • Relevant Coursework: Statistics, MATLAB Programming, Marketing management  AWARDS AND HONORS  • Scholarship: Terrapin Scholar, 2021 • Honorable mention: National Wealth Management Competition, 2016  WORK EXPERIENCE Hua Nan Securities Co., Ltd.                                                                                                                                                                  Taipei, Taiwan  Business Analyst                                                                                                                                                                    March 2021 – June 2021 • Created and conducted an investment analysis by extracting data from MSSQL and visualized portfolio performance, including return rate, investment allocation, and each trader's performance in bar charts, heat maps, highlight tables, and radar charts. • Developed automated report generation by a custom SQL query in Tableau, reducing daily manual work by 20%. • Investigated competitors' performance with Tableau, helping executives develop strategies and increasing market share by 10%. Taipei Fubon Commercial Bank Co., Ltd.                                                                                                                                            Taipei, Taiwan                                                     Assistant Relationship Manager                                                                                                                                      February 2019 – July 2020 • Completed syndicated loan of world's 3rd-largest passive component manufacturer in one month, which increased company's revenue by 5%. • Built a loan amortization schedule in Excel, increasing our team's productivity by 50%.  • Resolved 20+ customers' daily transactions and financial requirements via cross-functional communication.  Ernst & Young LLP                                                                                   Taipei, Taiwan   Auditor                                                                                                                                                                         July 2017 – October 2018 • Inspected clients' account data in Excel using Pivot Tables to validate the accuracy and reliability of the enterprises’ financial reports. • Led and taught summer interns how to investigate financial statements, including verification of income source and operating expenses.  ADDITIONAL CREDENTIALS • Languages: Chinese, English • Technical Skills: SQL, Tableau, Power BI, Python, R, HTML/CSS, Linux, Hadoop, Microsoft Word, Excel, PowerPoint, Outlook • Professional development: Big Data Analytics Training Course, Institute for Information Industry in Taiwan (September 2020) • Certificates: Tableau Desktop Specialist, Power BI Data Analyst Associate, Passed CFA Level I (December 2018)                                       • Projects: Google Maps Reviews Analytics - Designed and implemented a crawler with Selenium to collect and extract 10M+ restaurant reviews on Google Maps in Python. - Tokenized restaurant reviews and visualized outcomes to present keywords with word clouds using a static HTML page. • Professional affiliations: Business Analyst Professional, ModernAnalyst.com  • Interests: Reading, Traveling, Fitness and Nutrition, Data Visualization \""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "191db296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df['jobDescription'].apply(lambda x: score(resume,x))\n",
    "df.sort_values(by=['score'],ascending=False,inplace=True)\n",
    "df.to_excel(\"linkedIn_score.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "699e0468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>time</th>\n",
       "      <th>jobLink</th>\n",
       "      <th>jobDescription</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>Data Analyst - Commercial Real Estate / Privat...</td>\n",
       "      <td>The Horizon Group</td>\n",
       "      <td>New York City Metropolitan Area</td>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>The Horizon group is seeking a Data Analytic...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Data Analyst/SQL</td>\n",
       "      <td>iDfour</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>2022-08-29</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>The Data Analyst/BI Developer position is a ...</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Data Analyst - Remote</td>\n",
       "      <td>Smithfield Foods</td>\n",
       "      <td>Lisle, IL</td>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>Job Locations US-IL-Lisle Your Opportunity A...</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Virginia Green</td>\n",
       "      <td>Midlothian, VA</td>\n",
       "      <td>2022-08-30</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>Since 2004, Virginia Green has built the lar...</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>Data Analytics Analyst - Remote</td>\n",
       "      <td>Smithfield Foods</td>\n",
       "      <td>Lisle, IL</td>\n",
       "      <td>2022-08-30</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analyt...</td>\n",
       "      <td>Job Locations US-IL-Lisle Your Opportunity A...</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Synkriom</td>\n",
       "      <td>Hoboken, NJ</td>\n",
       "      <td>2022-09-09</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>Designation : Data Analyst- Engineer...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>eTeam</td>\n",
       "      <td>Santa Ana, CA</td>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>Reviewing reported Point of Sale (PO...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Analyst, Assessment &amp; Data (240209054)</td>\n",
       "      <td>Volusia County School District</td>\n",
       "      <td>DeLand, FL</td>\n",
       "      <td>2022-09-07</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/analyst-ass...</td>\n",
       "      <td>Position Code 240209054Title Code 22...</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Explore Job Search</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2022-08-15</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>To apply please visit:https://jobs.e...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Data Analyst - Telecommute</td>\n",
       "      <td>Explore Job Search</td>\n",
       "      <td>Indianapolis, IN</td>\n",
       "      <td>2022-08-15</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>To apply please visit:https://jobs.e...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "520  Data Analyst - Commercial Real Estate / Privat...   \n",
       "48                                    Data Analyst/SQL   \n",
       "365                              Data Analyst - Remote   \n",
       "118                                       Data Analyst   \n",
       "688                    Data Analytics Analyst - Remote   \n",
       "..                                                 ...   \n",
       "309                                       Data Analyst   \n",
       "730                                       Data Analyst   \n",
       "453             Analyst, Assessment & Data (240209054)   \n",
       "615                                       Data Analyst   \n",
       "677                         Data Analyst - Telecommute   \n",
       "\n",
       "                            company                         location  \\\n",
       "520               The Horizon Group  New York City Metropolitan Area   \n",
       "48                           iDfour                      Houston, TX   \n",
       "365                Smithfield Foods                        Lisle, IL   \n",
       "118                  Virginia Green                   Midlothian, VA   \n",
       "688                Smithfield Foods                        Lisle, IL   \n",
       "..                              ...                              ...   \n",
       "309                        Synkriom                      Hoboken, NJ   \n",
       "730                           eTeam                    Santa Ana, CA   \n",
       "453  Volusia County School District                       DeLand, FL   \n",
       "615              Explore Job Search                      Seattle, WA   \n",
       "677              Explore Job Search                 Indianapolis, IN   \n",
       "\n",
       "           time                                            jobLink  \\\n",
       "520  2022-09-06  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "48   2022-08-29  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "365  2022-09-12  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "118  2022-08-30  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "688  2022-08-30  https://www.linkedin.com/jobs/view/data-analyt...   \n",
       "..          ...                                                ...   \n",
       "309  2022-09-09  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "730  2022-08-18  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "453  2022-09-07  https://www.linkedin.com/jobs/view/analyst-ass...   \n",
       "615  2022-08-15  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "677  2022-08-15  https://www.linkedin.com/jobs/view/data-analys...   \n",
       "\n",
       "                                        jobDescription  score  \n",
       "520    The Horizon group is seeking a Data Analytic...   0.20  \n",
       "48     The Data Analyst/BI Developer position is a ...   0.19  \n",
       "365    Job Locations US-IL-Lisle Your Opportunity A...   0.19  \n",
       "118    Since 2004, Virginia Green has built the lar...   0.18  \n",
       "688    Job Locations US-IL-Lisle Your Opportunity A...   0.18  \n",
       "..                                                 ...    ...  \n",
       "309            Designation : Data Analyst- Engineer...   0.03  \n",
       "730            Reviewing reported Point of Sale (PO...   0.03  \n",
       "453            Position Code 240209054Title Code 22...   0.01  \n",
       "615            To apply please visit:https://jobs.e...   0.00  \n",
       "677            To apply please visit:https://jobs.e...   0.00  \n",
       "\n",
       "[997 rows x 7 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
